# OKR ML Pipeline with Kafka, Airflow, and MLflow

A modern machine learning pipeline that automatically updates ML models using streaming data from Kafka, orchestrated by Airflow, with experiment tracking via MLflow.

## ğŸ—ï¸ Project Structure

```
OKR/
â”œâ”€â”€ .github/                    # GitHub Actions workflows
â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ dags/                   # Airflow DAGs
â”‚   â”‚   â”œâ”€â”€ etl_pipeline.py     # ETL pipeline DAG
â”‚   â”‚   â”œâ”€â”€ model_training.py   # Model training DAG
â”‚   â”‚   â””â”€â”€ monitoring.py       # Monitoring DAG
â”‚   â”œâ”€â”€ models/                 # ML model functions
â”‚   â”‚   â”œâ”€â”€ training.py         # Model training logic
â”‚   â”‚   â””â”€â”€ evaluation.py       # Model evaluation
â”‚   â”œâ”€â”€ data/                   # Data processing functions
â”‚   â”‚   â”œâ”€â”€ preprocessing.py    # Data preprocessing
â”‚   â”‚   â””â”€â”€ streaming.py        # Kafka streaming functions
â”‚   â””â”€â”€ utils/                  # Utility functions
â”œâ”€â”€ data/                       # Data storage
â”‚   â”œâ”€â”€ raw/                    # Raw data
â”‚   â”œâ”€â”€ processed/              # Processed data
â”‚   â”œâ”€â”€ models/                 # Trained models
â”‚   â””â”€â”€ archive/                # Model versions
â”œâ”€â”€ configs/                    # Configuration files
â”œâ”€â”€ deploy/                     # Deployment configurations
â”œâ”€â”€ tests/                      # Test files
â”œâ”€â”€ scripts/                    # Utility scripts
â”œâ”€â”€ docker-compose.yml          # Docker services
â””â”€â”€ requirements.txt            # Python dependencies
```

## ğŸš€ Quick Start

### Prerequisites
- Docker and Docker Compose
- Python 3.8+
- Git

### 1. Clone and Setup
```bash
git clone <your-repo-url>
cd OKR
```

### 2. Start Services
```bash
# Build and start all services
docker-compose up -d --build

# Check service status
docker-compose ps
```

### 3. Access Services
- **Airflow**: http://localhost:8081 (airflow/airflow)
- **Kafka UI**: http://localhost:8085
- **API**: http://localhost:5001
- **Nginx**: http://localhost:80

## ğŸ”§ Services

### Core ML Pipeline
- **Kafka**: Stream data processing
- **Airflow**: Workflow orchestration
- **PostgreSQL**: Airflow metadata
- **MLflow**: Experiment tracking

### Data & API
- **Flask API**: REST API for predictions
- **Nginx**: Reverse proxy
- **Oracle**: Database (optional)

## ğŸ“Š DAGs

### ETL Pipeline
- Processes raw data
- Applies transformations
- Loads to processed storage

### Model Training
- Trains initial models
- Incremental model updates
- Performance evaluation

### Monitoring
- Model performance tracking
- Data quality checks
- Alert generation

## ğŸ³ Docker Services

All services are containerized and include:
- Health checks
- Proper networking
- Volume mounts for data persistence
- Environment-specific configurations

## ğŸ”„ CI/CD

GitHub Actions workflows for:
- Code quality checks
- Automated testing
- Deployment automation
- Branch protection

## ğŸ“ˆ Monitoring

- Real-time model performance
- Data pipeline health
- Resource utilization
- Error tracking

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“ License

[Your License Here]

## ğŸ†˜ Support

For issues and questions:
- Create an issue in GitHub
- Check the documentation
- Review deployment logs
